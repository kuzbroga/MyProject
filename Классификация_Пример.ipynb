{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOSmt/K00FVrZX0FOB7TIyY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Классификация в Scikit-Learn"],"metadata":{"id":"l7v3LtsJGl4Q"}},{"cell_type":"markdown","source":["Scikit-Learn даёт доступ ко множеству различных алгоритмов классификации."],"metadata":{"id":"r2D6E5HKGtqh"}},{"cell_type":"markdown","source":["**Метод k-ближайших соседей (K-Nearest Neighbors)**\n","\n","метод работает с помощью поиска кратчайшей дистанции между тестируемым объектом и ближайшими к нему классифицированным объектами из обучающего набора. Классифицируемый объект будет относится к тому классу, к которому принадлежит ближайший объект набора.\n","\n"],"metadata":{"id":"yst-hjj1GwUV"}},{"cell_type":"markdown","source":["**Классификатор дерева решений (Decision Tree Classifier)**\n","\n","классификатор разбивает данные на всё меньшие и меньшие подмножества на основе разных критериев, т. е. у каждого подмножества своя сортирующая категория. С каждым разделением количество объектов определённого критерия уменьшается.\n","\n","Классификация подойдёт к концу, когда сеть дойдёт до подмножества только с одним объектом. Если объединить несколько подобных деревьев решений, то получится так называемый Случайный Лес (англ. Random Forest)."],"metadata":{"id":"sfYP_YucHAD1"}},{"cell_type":"markdown","source":["**Наивный байесовский классификатор (Naive Bayes)**\n","\n","классификатор вычисляет вероятность принадлежности объекта к какому-то классу. Эта вероятность вычисляется из шанса, что какое-то событие произойдёт, с опорой на уже на произошедшие события.\n","\n","Каждый параметр классифицируемого объекта считается независимым от других параметров."],"metadata":{"id":"pxfGTAvEHK_s"}},{"cell_type":"markdown","source":["**Линейный дискриминантный анализ (Linear Discriminant Analysis)**\n","\n","метод работает путём уменьшения размерности набора данных, проецируя все точки данных на линию. Потом он комбинирует эти точки в классы, базируясь на их расстоянии от центральной точки.\n","\n","Метод хорошо подходит для данных с линейной зависимостью."],"metadata":{"id":"TOWc0kTWHPW3"}},{"cell_type":"markdown","source":["**Метод опорных векторов (Support Vector Machines)**\n","\n","Работа метода опорных векторов заключается в рисовании линии между разными кластерами точек, которые нужно сгруппировать в классы. С одной стороны линии будут точки, принадлежащие одному классу, с другой стороны — к другому классу.\n","\n","Классификатор будет пытаться увеличить расстояние между рисуемыми линиями и точками на разных сторонах, чтобы увеличить свою «уверенность» определения класса. Когда все точки построены, сторона, на которую они падают — это класс, которому эти точки принадлежат."],"metadata":{"id":"2znh30gLHYDg"}},{"cell_type":"markdown","source":["**Логистическая регрессия (Logistic Regression)**\n","\n","Логистическая регрессия выводит прогнозы о точках в бинарном масштабе — нулевом или единичном. Если значение чего-либо равно либо больше 0.5, то объект классифицируется в большую сторону (к единице). Если значение меньше 0.5 — в меньшую (к нулю).\n","\n","У каждого признака есть своя метка, равная только 0 или только 1. Логистическая регрессия является линейным классификатором и поэтому используется, когда в данных прослеживается какая-то линейная зависимость.\n","\n","---\n","\n"],"metadata":{"id":"LudKSMElHdtJ"}},{"cell_type":"markdown","source":["# Методология (на примере классификации ирисов)"],"metadata":{"id":"-NN6djXgG9Pt"}},{"cell_type":"markdown","source":["**Шаг 1.**\n","Импорт необходимых библиотек для использования различных классификаторов\n"],"metadata":{"id":"G0gBY19CH24z"}},{"cell_type":"code","execution_count":12,"metadata":{"id":"Vb8hhTNovd4N","executionInfo":{"status":"ok","timestamp":1731848000902,"user_tz":-180,"elapsed":254,"user":{"displayName":"Елена Кузнецова","userId":"01368276249782790589"}}},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.svm import SVC"]},{"cell_type":"markdown","source":["**Шаг 2.** Импорт дополнительных библиотек (работа с массивами, разделением выборки, расчета метрик)"],"metadata":{"id":"ov4nDqWkIWXk"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import accuracy_score\n"],"metadata":{"id":"sA6deEFywXFI","executionInfo":{"status":"ok","timestamp":1731848002117,"user_tz":-180,"elapsed":2,"user":{"displayName":"Елена Кузнецова","userId":"01368276249782790589"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["**Шаг 3**. Загружаем нужный CSV-файл в сессионное хранилище и определяем его в качестве датафрейма"],"metadata":{"id":"XjP-umYbKORh"}},{"cell_type":"code","source":["data = pd.read_csv('Iris.csv')\n","print(data.head(5))"],"metadata":{"id":"YSaYWkAvx5rH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Убираем ненужный столбец - в данном случае первый столбец с ID:"],"metadata":{"id":"DpmmEt-JCEoZ"}},{"cell_type":"code","source":["data.drop('Id', axis=1, inplace=True)"],"metadata":{"id":"sxL-G7t-LBrc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["С библиотекой Pandas «нарезаем» таблицу и выбираем определённые строки/столбцы с помощью функции iloc():"],"metadata":{"id":"QVtkUxhWLGjZ"}},{"cell_type":"code","source":["X = data.iloc[:,:-1].values\n","y = data['Species']"],"metadata":{"id":"iAAnHWqnzwVU","executionInfo":{"status":"ok","timestamp":1731848142663,"user_tz":-180,"elapsed":249,"user":{"displayName":"Елена Кузнецова","userId":"01368276249782790589"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["**Шаг 4**. Разделяем выборку на тренировочные и тестовые наборы, используя функцию train_test_split()"],"metadata":{"id":"ueEQz91GLTnN"}},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=100)"],"metadata":{"id":"SbRg1d4t0flJ","executionInfo":{"status":"ok","timestamp":1731848322951,"user_tz":-180,"elapsed":258,"user":{"displayName":"Елена Кузнецова","userId":"01368276249782790589"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["Вывод для подтверждения правильности данные:"],"metadata":{"id":"QR90wXjoLk32"}},{"cell_type":"code","source":["print(X_train)\n","print(y_train)"],"metadata":{"id":"USnoLlVX2IkC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Шаг 5**.\n","Создавать экземпляры классификаторов методом методом опорных векторов, k-ближайших соседей, наивным байесовским классификатором и деревом решений"],"metadata":{"id":"0E5Fk77zNG8s"}},{"cell_type":"code","source":["SVC_model = SVC()\n","KNN_model = KNeighborsClassifier(n_neighbors=5)\n","NB_model = GaussianNB()\n","Tree_model = DecisionTreeClassifier()"],"metadata":{"id":"7nKJQVgw2TwS","executionInfo":{"status":"ok","timestamp":1731848331981,"user_tz":-180,"elapsed":247,"user":{"displayName":"Елена Кузнецова","userId":"01368276249782790589"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["**Шаг 6**. обучим эти классификаторы:"],"metadata":{"id":"JVZZgGFCNiz6"}},{"cell_type":"code","source":["SVC_model.fit(X_train, y_train)\n","KNN_model.fit(X_train, y_train)\n","NB_model.fit(X_train, y_train)\n","Tree_model.fit(X_train, y_train)"],"metadata":{"id":"k9Ymgi_F2wIC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Шаг 7**. Делаем прогнозы и сохраняем результат в переменные:"],"metadata":{"id":"XSQJvN_uNvlk"}},{"cell_type":"code","source":["SVC_predict = SVC_model.predict(X_test)\n","KNN_predict = KNN_model.predict(X_test)\n","NB_predict = NB_model.predict(X_test)\n","Tree_predict = Tree_model.predict(X_test)"],"metadata":{"id":"n8I9Z9h_3HHY","executionInfo":{"status":"ok","timestamp":1731848546995,"user_tz":-180,"elapsed":268,"user":{"displayName":"Елена Кузнецова","userId":"01368276249782790589"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":["**Шаг 8**. Находим accuracy для обученных классификаторов для тестовой выборки:"],"metadata":{"id":"v562UVjrN8My"}},{"cell_type":"code","source":["print(accuracy_score(SVC_predict, y_test))\n","print(accuracy_score(KNN_predict, y_test))\n","print(accuracy_score(NB_predict, y_test))\n","print(accuracy_score(Tree_predict, y_test))\n"],"metadata":{"id":"2tkleCdY282K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Шаг 9**. Отражаем матрицу неточности для классификаторов"],"metadata":{"id":"3vD-TQHIOS4S"}},{"cell_type":"code","source":["print(confusion_matrix(SVC_predict, y_test))\n","print(confusion_matrix(KNN_predict, y_test))\n","print(confusion_matrix(NB_predict, y_test))\n","print(confusion_matrix(Tree_predict, y_test))"],"metadata":{"id":"h5H_C5QK8x1E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Шаг 10**. Отражаем отчёт о классификации для классификаторов"],"metadata":{"id":"EIh-AJ9FOfmx"}},{"cell_type":"code","source":["print(classification_report(SVC_predict, y_test))\n","print(classification_report(KNN_predict, y_test))\n","print(classification_report(NB_predict, y_test))\n","print(classification_report(Tree_predict, y_test))"],"metadata":{"id":"Gr97S_FH8vuj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","# Данные из отдельных столбцов\n","sepal_length = data['SepalLengthCm'].tolist()\n","sepal_width = data['SepalWidthCm'].tolist()\n","petal_length = data['PetalLengthCm'].tolist()\n","petal_width = data['PetalWidthCm'].tolist()\n","#\tСтроим графики no проекциям данных\n","# Учитываем, что каждые 50 типов ирисов идут последовательно\n","# 1 проекция - ширина и длина чашелистника\n","setosa, = plt.plot(sepal_length[:50], sepal_width[:50], \"ro\", label=\"Setosa\")\n","versicolor, = plt.plot(sepal_length[50:100], sepal_width[50:100], \"v\", label ='Versicolor')\n","virginica, = plt.plot(sepal_length[100:150], sepal_width[100:150], 'bs', label='Verginica')\n","plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n","plt.xlabel(\"Sepal Length\")\n","plt.ylabel('Sepal Width')\n","plt.show()"],"metadata":{"id":"0BQ1yDLtA3Fn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1 проекция - ширина и длина лепестка\n","setosa, = plt.plot(petal_length[:50], petal_width[:50], \"ro\", label=\"Setosa\")\n","versicolor, = plt.plot(petal_length[50:100], petal_width[50:100], \"v\", label ='Versicolor')\n","virginica, = plt.plot(petal_length[100:150], petal_width[100:150], 'bs', label='Verginica')\n","plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n","plt.xlabel(\"Sepal Length\")\n","plt.ylabel('Sepal Width')\n","plt.show()"],"metadata":{"id":"5xyrZ8T-VD_T"},"execution_count":null,"outputs":[]}]}